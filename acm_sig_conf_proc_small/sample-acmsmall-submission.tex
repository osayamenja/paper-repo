%%
%% This is file `sample-acmsmall-submission.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `acmsmall-submission')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-acmsmall-submission.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command.
\documentclass[acmsmall,screen,review,anonymous]{acmart}
%% To ensure 100% compatibility, please check the white list of
%% approved LaTeX packages to be used with the Master Article Template at
%% https://www.acm.org/publications/taps/whitelist-of-latex-packages 
%% before creating your document. The white list page provides 
%% information on how to submit additional LaTeX packages for 
%% review and adoption.
%% Fonts used in the template cannot be substituted; margin 
%% adjustments are not allowed.

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2023}
\acmYear{2023}
\acmDOI{}

%%
%% These commands are for a JOURNAL article.
\acmConference[SIGMETRICS '24]{ACM SIGMETRICS Conference}{June 10--14,
  2024}{Venice, Italy}
\acmVolume{}
\acmNumber{}
\acmArticle{}
\acmMonth{}

%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{TBD}

\author{Jonathan Aimuyo}
\email{oja7@cornell.edu}
\affiliation{%
  \institution{Cornell University}
  \streetaddress{402 Gates Hall}
  \city{Ithaca}
  \state{NY}
  \country{USA}
  \postcode{14853-7501}
}

\author{Rachee Singh}
\email{rachee@cs.cornell.edu}
\affiliation{%
  \institution{Cornell University}
  \streetaddress{402 Gates Hall}
  \city{Ithaca}
  \state{NY}
  \country{USA}
  \postcode{14853-7501}
}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Aimuyo and Singh}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  Through sparsely-activated computation, the Mixture-of-Experts (MoE) architecture
  mitigates the skyrocketing scaling costs, for training and inference, and power consumption of larger Deep Learning models.
  Existing work demonstrates that this architecture achieves these savings without compromising latency or model accuracy.
  However, MoE computation necessitates frequent collective communication that poses
  significant overhead, especially at scale.
  \textbf{Analytically and empirically quantifying this overhead in MoE transformers across NLG and CV
  workloads is the motivation for this paper}
  \footnote{Training Code available here: \url{https://github.com/osayamenja/Megatron-DeepSpeed}}
  \footnote{Jupyter Notebook for reproducing all results available here: \url{https://github.com/osayamenja/DataCruncher}}
  We validate the hypothesis that this overhead bottlenecks MoE computation at scale,
  even for high-bandwidth multi-node interconnects within a supercomputer.

  In particular, using NVIDIA NSight and the \textbf{Perlmutter} supercomputer, we profile the training and inference of GPT-3 MoE 6.7B (only training),
  Mixtral 8x7B, and Swin-MoE, arriving at \textbf{12} takeaways recommending best practices for
  distributed MoE computation and suggestions for future research work.
  We discover that the overhead due to synchronous \emph{All-to-All} in these MoE models bottlenecks a training step by
  up to \textbf{60\%}.
  Further, we demonstrate that synchronous \emph{All-to-All} is highly \emph{sensitive} to delays in a model's critical
  path by showing its execution times exhibit a long-tail distribution with a \textbf{4.9X} mean slowdown
  independent of any system or software bottlenecks, even within NVLink interconnected GPUs.
  We reproduce this finding in the Perlmutter with a state-of-the art inter-node interconnect as well.
  In addition, using our empirical findings, we contrive and leverage a minuscule predictive model based on Amdahl's law
  to motivate our suggestions for future research in collective algorithms and network interconnects to tackle
  this communication overhead.
\end{abstract}

%%
%% The code below is generated by the tool at https://dl.acm.org/ccs
%%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10010147.10010257</concept_id>
       <concept_desc>Computing methodologies~Machine learning</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10010147.10010919</concept_id>
       <concept_desc>Computing methodologies~Distributed computing methodologies</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10003033.10003079.10003080</concept_id>
       <concept_desc>Networks~Network performance modeling</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Machine learning}
\ccsdesc[500]{Computing methodologies~Distributed computing methodologies}
\ccsdesc[500]{Networks~Network performance modeling}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Mixture-of-Experts; Collective Communication; GPUs}

\received{28 January 2024}
%\received[revised]{12 March 2009}
%\received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}\label{sec:introduction}
Since the advent of the Transformer~\cite{NEURIPS2017_3f5ee243} in 2017, existing trends in language model complexity
indicate a steep exponential increase~\cite{DBLP:journals/corr/abs-2201-11990} in model parameters.
These larger Transformer models achieve significantly better performance~\cite{DBLP:journals/corr/abs-2005-14165},
hence the appeal to scale their size continuously.
However, this scale demands exorbitant compute resources,
necessitating a distributed setting as memory requirements exceed the capacity of any
single accelerator (GPU, TPU, FPGA, etc).
This interplay begs the question:
\emph{how do we sidestep the high energy and hardware costs of scaling Large Transformer models?}

Current work shows the Mixture-of-Experts architecture is a productive answer to this question.
For example, the DeepSpeed team of Microsoft showed 5x less training time and costs
and 10X lower inference latency for language MoE models with billions of parameters~\cite{pmlr-v162-rajbhandari22a}.
Likewise, at 1.2 trillion parameters, Google's MoE model GLaM~\cite{pmlr-v162-du22c}, compared to GPT-3,
consumes a third of the energy and half the FLOPs during training and inference, respectively.
As of yet, the versatility of the MoE architecture has seen it applied to other workloads,
such as vision~\cite{NEURIPS2021_48237d9f} and multimodal tasks~\cite{NEURIPS2022_3e67e84a}.

On the other hand, this architecture introduces new algorithmic and systems challenges,
such as load balancing across experts, training instability and
collective communication overhead~\cite{ShazeerMMDLHD17, zoph2022stmoe}.
Existing literature <4 sources> points out this overhead due to the synchronous \emph{All-to-All} communication
and propose solutions.
However, to the best of our knowledge, no existing work quantifies this overhead,
including its properties at the scale of hundreds to thousands of GPUs.
%In addressing this gap, we make the following novel contributions:
%\begin{itemize}
%  \item Analytically prove a lower and upper bound for the number of \emph{All-to-All} necessary per training step in the MoE architecture.
%  \item Quantify \emph{All-to-All} overhead for Distributed MoE in the multi-node setting.
%  \item Provide a model, based on Amdahl's law, that shows projected speedup as a function of reducing the overhead of \emph{All-to-All} in the multi-node and optimizing compute for single-node.
%\end{itemize}

%The rest of the paper is as follows: brief introduction to the MoE architecture, experimental setup, methodology, results/analysis and conclusion.

\section{Background}\label{sec:background}
MoE architecture.

%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
\begin{acks}
To Robert, for the bagels and explaining CMYK and color spaces.
\end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{biblio}

%%
%% If your work has an appendix, this is the place to put it.
\appendix

\end{document}
\endinput
%%
%% End of file `sample-acmsmall-submission.tex'.
