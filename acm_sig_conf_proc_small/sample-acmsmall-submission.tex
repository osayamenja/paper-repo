%%
%% This is file `sample-acmsmall-submission.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `acmsmall-submission')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-acmsmall-submission.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command.
\documentclass[acmsmall,screen,review,anonymous]{acmart}
%% To ensure 100% compatibility, please check the white list of
%% approved LaTeX packages to be used with the Master Article Template at
%% https://www.acm.org/publications/taps/whitelist-of-latex-packages 
%% before creating your document. The white list page provides 
%% information on how to submit additional LaTeX packages for 
%% review and adoption.
%% Fonts used in the template cannot be substituted; margin 
%% adjustments are not allowed.

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2023}
\acmYear{2023}
\acmDOI{}

%%
%% These commands are for a JOURNAL article.
\acmConference[SIGMETRICS '24]{ACM SIGMETRICS Conference}{June 10--14,
  2024}{Venice, Italy}
\acmVolume{}
\acmNumber{}
\acmArticle{}
\acmMonth{}

%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{TBD}

\author{Jonathan Aimuyo}
\email{oja7@cornell.edu}
\affiliation{%
  \institution{Cornell University}
  \streetaddress{402 Gates Hall}
  \city{Ithaca}
  \state{NY}
  \country{USA}
  \postcode{14853-7501}
}

\author{Rachee Singh}
\email{rachee@cs.cornell.edu}
\affiliation{%
  \institution{Cornell University}
  \streetaddress{402 Gates Hall}
  \city{Ithaca}
  \state{NY}
  \country{USA}
  \postcode{14853-7501}
}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Aimuyo and Singh}

\renewcommand{\ata}{\verb|AlltoAll|}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  Through sparsely-activated computation, the Mixture-of-Experts (MoE) architecture
  mitigates the skyrocketing scaling costs, for training and inference, and power consumption of larger Deep Learning models.
  Existing work demonstrates that this architecture achieves these savings without compromising latency or model accuracy.
  However, MoE computation necessitates frequent collective communication that poses
  significant overhead, especially at scale.
  \textbf{Analytically and empirically quantifying this overhead in MoE transformers across NLG and CV
  workloads is the motivation for this paper}
  \footnote{Training Code available here: \url{https://github.com/osayamenja/Megatron-DeepSpeed}}
  \footnote{Jupyter Notebook for reproducing all results available here: \url{https://github.com/osayamenja/DataCruncher}}
  We validate the hypothesis that this overhead bottlenecks MoE computation at scale,
  even for high-bandwidth multi-node interconnects within a supercomputer.

  In particular, using NVIDIA NSight and the \textbf{Perlmutter} supercomputer, we profile the training and inference of GPT-3 MoE 6.7B (only training),
  Mixtral 8x7B, and Swin-MoE, arriving at \textbf{12} takeaways recommending best practices for
  distributed MoE computation and suggestions for future research work.
  We discover that the overhead due to synchronous \emph{All-to-All} in these MoE models bottlenecks a training step by
  up to \textbf{60\%}.
  Further, we demonstrate that synchronous \emph{All-to-All} is highly \emph{sensitive} to delays in a model's critical
  path by showing its execution times exhibit a long-tail distribution with a \textbf{4.9X} mean slowdown
  independent of any system or software bottlenecks, even within NVLink interconnected GPUs.
  We reproduce this finding in the Perlmutter with a state-of-the art inter-node interconnect as well.
  In addition, using our empirical findings, we contrive and leverage a minuscule predictive model based on Amdahl's law
  to motivate our suggestions for future research in collective algorithms and network interconnects to tackle
  this communication overhead.
\end{abstract}

%%
%% The code below is generated by the tool at https://dl.acm.org/ccs
%%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10010147.10010257</concept_id>
       <concept_desc>Computing methodologies~Machine learning</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10010147.10010919</concept_id>
       <concept_desc>Computing methodologies~Distributed computing methodologies</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10003033.10003079.10003080</concept_id>
       <concept_desc>Networks~Network performance modeling</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Machine learning}
\ccsdesc[500]{Computing methodologies~Distributed computing methodologies}
\ccsdesc[500]{Networks~Network performance modeling}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Mixture-of-Experts, Collective Communication, GPUs}

\received{\today}
%\received[revised]{12 March 2009}
%\received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}\label{sec:introduction}
Since the advent of the Transformer~\cite{NEURIPS2017_3f5ee243} in 2017, existing trends in language model complexity
indicate a steep exponential increase~\cite{DBLP:journals/corr/abs-2201-11990} in model parameters.
These larger Transformer models achieve significantly better performance~\cite{DBLP:journals/corr/abs-2005-14165},
hence the appeal to scale their size continuously.
However, this scale demands exorbitant compute resources,
necessitating a distributed setting as memory requirements exceed the capacity of any
single accelerator (GPU, TPU, FPGA, etc)~\cite{DBLP:journals/corr/abs-2201-11990}
This interplay begs the question:
\emph{how do we sidestep the high energy and hardware costs of scaling Large Transformer models?}

Current work shows the Mixture-of-Experts (MoE) architecture is a productive answer to this question.
In a nutshell, the
For example, the recently released Mixtral 8x7B, a multilingual MoE model replacing each Feed Forward Network (FFN) per layer
with 8 experts, outperforms LLaMA-2 70B and GPT 3.5 on seven benchmarks~\cite{mixtral8x7B}.
Interestingly, Mixtral achieves this better performance at the inference cost of 12.9B parameters,
subset activated per token, instead of its actual 46.7B parameters.
Another example comes from the DeepSpeed team of Microsoft who showed 5x less training time for GPT-3 MoE 1.3B and
7.3X faster inference for a trillion-parameter MoE model~\cite{pmlr-v162-rajbhandari22a}.
Likewise, at 1.2 trillion parameters, Google's MoE model GLaM, compared to GPT-3,
consumes a third of the energy and half the FLOPs during training and inference, respectively~\cite{pmlr-v162-du22c}.
As of yet, the versatility of transformer MoE has seen it successfully applied to other workloads,
such as vision~\cite{NEURIPS2021_48237d9f}, achieving 10.5x cheaper inference~\cite{puigcerver2023sparse},
multimodal tasks~\cite{NEURIPS2022_3e67e84a}, and production recommender systems~\cite{DBLP:journals/corr/abs-2108-04690}.

On the other hand, this architecture introduces new algorithmic and systems challenges,
such as load balancing across experts~\cite{ShazeerMMDLHD17}, training instability~\cite{NEURIPS2022_3e67e84a},
expert capacity restriction leading to token dropping~\cite{gale2022megablocks},
and collective communication overhead~\cite{DBLP:journals/corr/abs-2006-16668}.
In this paper, we explicitly study the communication overhead due to \emph{All-to-All} (typeset as \verb|all2all|
henceforth) which occur at the \verb|dispatch|, after routing tokens, and \verb|combine| stages within the MoE layer
(\S \ref{sec:background}).

\textbf{\emph{Motivation \& Related Work}}: In addition to scaling, there has also been greater drive towards
identifying and tackling systems or software bottlenecks via profiling and analyzing the operational behavior of
accelerators, memory or interconnects during training or inference.
This methodology motivates questions like \emph{x} and \emph{y} which lead to pioneering research work
such as vLLM~\cite{kwon2023efficient} and Tutel~\cite{hwang2023tutel}.
Specifically within MoE computation, we find related work applies this technique to identify \verb|all2all| overhead;
foe et al.~\cite{10.1145/3603269.3604869} find x, and so on.

However, to the best of our knowledge, they do not use the state-of-the-art CUDA profiling systems~\cite{10.1145/3578244.3583736}:
NVIDIA NSight Systems~\cite{nsys} and Nsight Compute~\cite{ncu}, which provides \emph{all} CUPTI metrics and x,
de-facto Pytorch profiler\cite{} which exposes only a relevant subset of CUPTI metrics generally satisfying Deep Learning profiling use-cases.
Understandably so, as the Nsight suite requires significant domain knowledge to leverage its vast capabilities fully,
hence why they are more prevalent in the High-Performance Computing (HPC)~\cite{} and performance engineering~\cite{10.1145/3578244.3583736} communities.

\textbf{\emph{Contributions}}: More generally, we first do y.

\textbf{\emph{Summary}}:
%In addressing this gap, we make the following novel contributions:
%\begin{itemize}
%  \item Analytically prove a lower and upper bound for the number of \emph{All-to-All} necessary per training step in the MoE architecture.
%  \item Quantify \emph{All-to-All} overhead for Distributed MoE in the multi-node setting.
%  \item Provide a model, based on Amdahl's law, that shows projected speedup as a function of reducing the overhead of \emph{All-to-All} in the multi-node and optimizing compute for single-node.
%\end{itemize}

%The rest of the paper is as follows: brief introduction to the MoE architecture, experimental setup, methodology, results/analysis and conclusion.

\section{Background}\label{sec:background}
MoE architecture.

%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
\begin{acks}
To Robert, for the bagels and explaining CMYK and color spaces.
\end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{biblio}

%%
%% If your work has an appendix, this is the place to put it.
\appendix

\end{document}
\endinput
%%
%% End of file `sample-acmsmall-submission.tex'.
